144	172	200	228
153	181	209	237
162	190	218	246
171	199	227
180	208	236
189	217	245
198	226
207	235
216	244								'
test <- gsub('\t', ' ', test)
test <- gsub('\n', ' ', test)
test2 <- strsplit(test, split = ' ')
test2 <- unlist(test2)
test2 <- as.integer(test2)
test2 <- sort(test2)
test2 <- test2[!duplicated(test2)]
test2 <- test2[test2 < 214]
test2
setwd("~/Documents/textlab")
library(magrittr)
library(dplyr)
library(tidytext)
library(dbscan)
library(tm)
library(readr)
load("./data/processed_6gram__AHCA_LocalNews_072517_080917_processed_.RData")
ids <- lapply(processed_6gram__AHCA_LocalNews_072517_080917_processed_, function(x) {
return(x$id[1])
}) %>%
unlist() %>%
unique()
relevant_ids <- ids[grepl("07-25-2017", ids)] %>% unique()
ngram <-  processed_6gram__AHCA_LocalNews_072517_080917_processed_ %>%
do.call('rbind', .) %>%
filter(id %in% relevant_ids)
ngram <- ngram %>%
group_by(id) %>%
mutate(index = 1:n()) %>%
ungroup()
countNgrams <- ngram %>%
mutate(id_unique = str_extract(id, "^[0-9a-zA-Z]+")) %>%
group_by(id_unique) %>%
select(id_unique, ngram) %>%
distinct() %>%
group_by(ngram) %>%
summarise(count = n()) %>%
ungroup
countNgrams <- merge(countNgrams, ngram, by="ngram") %>%
arrange(id, index)
countNgrams$segmentId <- countNgrams$id
countNgrams$num_id <- as.factor(countNgrams$id) %>% as.numeric
content <- data.frame(num_id = integer(0),
contentId = character(0),
first = integer(0),
last = integer(0),
start_line = integer(0),
end_line = integer(0),
start_timestamp = integer(0),
end_timestamp = integer(0),
station_id = integer(0))
pdf("./data/segments/plot.pdf")
for (i in 1:1000) {
print(i)
mat <- na.omit(cbind(1:length(countNgrams$count[countNgrams$num_id == i]),
as.character(cut(countNgrams$count[countNgrams$num_id == i],
breaks = c(-Inf, 2, 5, Inf),
labels = c(0, 0.9, 1))) %>% as.numeric()))
mat <- as.data.frame(mat) %>%
filter(V2 != 0) %>%
as.matrix
#
# ifelse(countNgrams$count[countNgrams$num_id == i] >= 5,
#        1,
#        0)))
# su <- summary(mat[,2])
if (nrow(mat) == 0) {
next
}
# plot(mat)
# abline(h = su[2], col="blue")
# abline(h = su[4], col="red")
# abline(h = su[5], col="green")
# if (nrow(mat) == 0) {
#   next
# }
#
# perform dbscan to get clusters of content =>. segments
db <- dbscan(mat, 20, 5)
group <- data.frame(x = mat[,1],
y = mat[,2],
cluster = db$cluster) %>%
filter(cluster!=0)
if (nrow(group) == 0) {
next
}
group <- cbind(countNgrams[countNgrams$num_id == i,][group$x,], group)
# plot(group$x, group$y, col=factor(group$cluster))
# title(countNgrams$segmentId[countNgrams$num_id == i][1])
#
#
content <- group %>%
filter(cluster != 0) %>%
group_by(cluster) %>%
summarise(num_id = i,
contentId = paste(i, first(cluster)),
first = min(x),
last = max(x),
start_line = linenumber[which(x==min(x))],
end_line = linenumber[which(x==max(x))],
start_timestamp = timestamp[which(x==min(x))],
end_timestamp = timestamp[which(x==max(x))],
station_id = first(id),
count = n()) %>%
filter(count > 10) %>%   # only keep cluster if there are more than 10 words
select(num_id, contentId, first, last, start_line, end_line, start_timestamp, end_timestamp, station_id) %>%
rbind(content, .)
#
# print(i)
}
countNgrams <- ngram %>%
mutate(id_unique = str_extract(id, "^[0-9a-zA-Z]+")) %>%
group_by(id_unique) %>%
select(id_unique, ngram) %>%
distinct() %>%
group_by(ngram) %>%
summarise(count = n()) %>%
ungroup
library(stringr)
countNgrams <- ngram %>%
mutate(id_unique = str_extract(id, "^[0-9a-zA-Z]+")) %>%
group_by(id_unique) %>%
select(id_unique, ngram) %>%
distinct() %>%
group_by(ngram) %>%
summarise(count = n()) %>%
ungroup
countNgrams <- merge(countNgrams, ngram, by="ngram") %>%
arrange(id, index)
countNgrams$segmentId <- countNgrams$id
countNgrams$num_id <- as.factor(countNgrams$id) %>% as.numeric
content <- data.frame(num_id = integer(0),
contentId = character(0),
first = integer(0),
last = integer(0),
start_line = integer(0),
end_line = integer(0),
start_timestamp = integer(0),
end_timestamp = integer(0),
station_id = integer(0))
pdf("./data/segments/plot.pdf")
for (i in 1:1000) {
print(i)
mat <- na.omit(cbind(1:length(countNgrams$count[countNgrams$num_id == i]),
as.character(cut(countNgrams$count[countNgrams$num_id == i],
breaks = c(-Inf, 2, 5, Inf),
labels = c(0, 0.9, 1))) %>% as.numeric()))
mat <- as.data.frame(mat) %>%
filter(V2 != 0) %>%
as.matrix
#
# ifelse(countNgrams$count[countNgrams$num_id == i] >= 5,
#        1,
#        0)))
# su <- summary(mat[,2])
if (nrow(mat) == 0) {
next
}
# plot(mat)
# abline(h = su[2], col="blue")
# abline(h = su[4], col="red")
# abline(h = su[5], col="green")
# if (nrow(mat) == 0) {
#   next
# }
#
# perform dbscan to get clusters of content =>. segments
db <- dbscan(mat, 20, 5)
group <- data.frame(x = mat[,1],
y = mat[,2],
cluster = db$cluster) %>%
filter(cluster!=0)
if (nrow(group) == 0) {
next
}
group <- cbind(countNgrams[countNgrams$num_id == i,][group$x,], group)
# plot(group$x, group$y, col=factor(group$cluster))
# title(countNgrams$segmentId[countNgrams$num_id == i][1])
#
#
content <- group %>%
filter(cluster != 0) %>%
group_by(cluster) %>%
summarise(num_id = i,
contentId = paste(i, first(cluster)),
first = min(x),
last = max(x),
start_line = linenumber[which(x==min(x))],
end_line = linenumber[which(x==max(x))],
start_timestamp = timestamp[which(x==min(x))],
end_timestamp = timestamp[which(x==max(x))],
station_id = first(id),
count = n()) %>%
filter(count > 10) %>%   # only keep cluster if there are more than 10 words
select(num_id, contentId, first, last, start_line, end_line, start_timestamp, end_timestamp, station_id) %>%
rbind(content, .)
#
# print(i)
}
content$length <- content$last - content$first
content %<>%
filter(length < 150)
content %<>%
mutate(first = first - 20,
first = ifelse(first < 0, 0, first))
getNgramsToText <- function(ngramdata, id_to_search, first, last) {
# list of ngrams
ngram.list <- ngramdata %>%
dplyr::filter(id == id_to_search) %$%
ngram[first:last]
# take first word  of each ngrams and paste them together
#  and the entire string of the last ngram
ngram.list[-length(ngram.list)] %>%
lapply(FUN = function(x) {
strsplit(x, ' ') %>%
unlist() %>%
.[1]
}) %>%
paste(collapse = " ") %>%
paste(ngram.list[length(ngram.list)])
}
getNgramsToVector <- function(ngramdata, id_to_search, first, last) {
# list of ngrams
ngramdata %>%
dplyr::filter(id == id_to_search) %$%
ngram[first:last]
}
content$text <- ""
for (i in 1:nrow(content)) {
content$text[i] <- getNgramsToText(ngram, content$station_id[i], content$first[i], content$last[i])
}
content_list <- list()
for (i in 1:nrow(content)) {
content_list[[i]] <- getNgramsToVector(ngram, content$station_id[i], content$first[i], content$last[i])
print(i)
}
segmentSimilarityNgram<- function(text.x, text.y) {
# split.x <- str_split(text.x, " ") %>% unlist
# split.y <- str_split(text.y, " ") %>% unlist
# ngram.x <- lapply(1:(length(split.x)-2), function(x) {
#   split.x[x:(x+2)] %>% paste(collapse=" ")
# }) %>% unlist
# ngram.y <- lapply(1:(length(split.y)-2), function(y) {
#   split.y[y:(y+2)]  %>% paste(collapse=" ")
# }) %>% unlist
sizeIntersection <- length(intersect(text.x, text.y))
similarity <- sizeIntersection/length(text.x)
return(similarity)
}
similarityScoresNgram <- matrix(nrow = nrow(content), ncol = nrow(content))
for (i in 1:nrow(content)) {
for (j in 1:nrow(content)) {
if (i==j) {
similarityScoresNgram[i,j] <- NA
next
}
text.x <- content_list[[i]]
text.y <- content_list[[j]]
similarityScoresNgram[i,j] <- segmentSimilarityNgram(text.x, text.y)
}
print(paste(i, j))
}
ggg <- similarityScoresNgram > 0.7
library(igraph)
g1 <- graph_from_adjacency_matrix(ggg, mode = "directed")
cluster <- clusters(g1, mode = "weak")$membership
content$cluster <- cluster
segments <- content[order(content$cluster), c("num_id", "contentId", "text", "cluster", "start_line",
"end_line", "start_timestamp", "end_timestamp", "station_id")]
segments$segmentLength <- str_extract_all(segments$text, "\\s") %>% lapply(FUN = length) %>% unlist
uniqueSegments <- segments %>%
group_by(cluster) %>%
arrange(desc(segmentLength)) %>%
summarise(sampleText = text[1],
num_transcripts=n()) %>%
arrange(desc(num_transcripts))
View(uniqueSegments)
ggg <- similarityScoresNgram > 0.4
library(igraph)
g1 <- graph_from_adjacency_matrix(ggg, mode = "directed")
cluster <- clusters(g1, mode = "weak")$membership
content$cluster <- cluster
clusters(g1, mode = "weak")
ggg <- similarityScoresNgram > 0.3
library(igraph)
g1 <- graph_from_adjacency_matrix(ggg, mode = "directed")
cluster <- clusters(g1, mode = "weak")$membership
clusters(g1, mode = "weak")
content$cluster <- cluster
plot(g1, mode = "weak", edge.arrow.size=0.1,
vertex.size=5,
vertex.color=cluster,
vertex.label=NA)
dev.off()
dev.off()
plot(g1, mode = "weak", edge.arrow.size=0.1,
vertex.size=5,
vertex.color=cluster,
vertex.label=NA)
segments <- content[order(content$cluster), c("num_id", "contentId", "text", "cluster", "start_line",
"end_line", "start_timestamp", "end_timestamp", "station_id")]
segments$url <- ""
segments <- merge(segments, uniqueSegments %>% select(cluster, num_transcripts), by="cluster")
segments %<>% arrange(desc(num_transcripts), cluster)
json <- list()
library(jsonlite)
library(readr)
for (i in 1:nrow(uniqueSegments)) {
segment_list <- split(segments[segments$cluster == i, ] , seq(nrow(segments[segments$cluster == i, ]))) %>%
lapply(function(x) {return(x %>% as.list())}) %>%
unname() %>%
list() # cover with a "master list"
json[[i]] <- c("cluster_message"=uniqueSegments$sampleText[uniqueSegments$cluster == i], "stations"=segment_list)
}
json <- list("date"="07-25-2017", "clusters"=json)
json %>% jsonlite::toJSON(pretty = T, auto_unbox = T) %>% write_lines('./data/segments/07-25-2017_extracted-03-06-18.json')
segments <- content[order(content$cluster), c("num_id", "contentId", "text", "cluster", "start_line",
"end_line", "start_timestamp", "end_timestamp", "station_id")]
uniqueSegments <- segments %>%
group_by(cluster) %>%
arrange(desc(segmentLength)) %>%
summarise(sampleText = text[1],
num_transcripts=n()) %>%
arrange(desc(num_transcripts))
segments$segmentLength <- str_extract_all(segments$text, "\\s") %>% lapply(FUN = length) %>% unlist
uniqueSegments <- segments %>%
group_by(cluster) %>%
arrange(desc(segmentLength)) %>%
summarise(sampleText = text[1],
num_transcripts=n()) %>%
arrange(desc(num_transcripts))
write.csv(segments, file="./data/segments/07-25-2017_extracted-03-01.csv", row.names = F)
write.csv(uniqueSegments, file="./data/segments/07-25-2017_extracted-03-01_unique.csv", row.names = F)
library(jsonlite)
library(readr)
json <- list()
for (i in 1:nrow(uniqueSegments)) {
segment_list <- split(segments[segments$cluster == i, ] , seq(nrow(segments[segments$cluster == i, ]))) %>%
lapply(function(x) {return(x %>% as.list())}) %>%
unname() %>%
list() # cover with a "master list"
json[[i]] <- c("cluster_message"=uniqueSegments$sampleText[uniqueSegments$cluster == i], "stations"=segment_list)
}
json <- list("date"="07-25-2017", "clusters"=json)
json %>% jsonlite::toJSON(pretty = T, auto_unbox = T) %>% write_lines('./data/segments/07-25-2017_extracted-03-06-18.json')
segments <- content[order(content$cluster), c("num_id", "contentId", "text", "cluster", "start_line",
"end_line", "start_timestamp", "end_timestamp", "station_id")]
segments$url <- ""
segments$segmentLength <- str_extract_all(segments$text, "\\s") %>% lapply(FUN = length) %>% unlist
uniqueSegments <- segments %>%
group_by(cluster) %>%
arrange(desc(segmentLength)) %>%
summarise(sampleText = text[1],
num_transcripts=n()) %>%
arrange(desc(num_transcripts))
json <- list()
for (i in 1:nrow(uniqueSegments)) {
cluster_to_process = uniqueSegments$cluster[i]
segment_list <- split(segments[segments$cluster == cluster_to_process, ] ,
seq(nrow(segments[segments$cluster == cluster_to_process, ]))) %>%
lapply(function(x) {return(x %>% as.list())}) %>%
unname() %>%
list() # cover with a "master list"
json[[i]] <- c("cluster_message"=uniqueSegments$sampleText[i], "stations"=segment_list)
}
json <- list("date"="07-25-2017", "clusters"=json)
json %>% jsonlite::toJSON(pretty = T, auto_unbox = T) %>% write_lines('./data/segments/07-25-2017_extracted-03-06-18.json')
ggg <- similarityScoresNgram > 0.6
library(igraph)
g1 <- graph_from_adjacency_matrix(ggg, mode = "directed")
cluster <- clusters(g1, mode = "weak")$membership
content$cluster <- cluster
plot(g1, mode = "weak", edge.arrow.size=0.1,
vertex.size=5,
vertex.color=cluster,
vertex.label=NA)
getVideoQuery <- function(station_id, start_timestamp) {
timestamps <- lubridate::mdy_hms(segments$start_timestamp)
station_id %>%
stringr::str_extract('.*?(?=-)') %>%
paste(lubridate::month(timestamps) %>%
str_pad(2, "left", "0"),
lubridate::day(timestamps) %>%
str_pad(2, "left", "0"), lubridate::year(timestamps) %>% substr(3,4), sep = "") %>%
{paste('FileName LIKE "%', ., '%" OR ', sep = "")} %>%
unique() %>%
paste(collapse = " ")
}
write_lines(getVideoQuery(segments$station_id, segments$start_timestamp), "test.txt")
videos <- read_csv("./data/segments/072517_videos.csv")
names(videos) <- c("filename", "dirpath", "starttime", "endtime", "segmentid")
segments <- content[order(content$cluster), c("num_id", "contentId", "text", "cluster", "start_line",
"end_line", "start_timestamp", "end_timestamp", "station_id")]
segments$url <- ""
for (i in 1:nrow(segments)) {
segments$url[i] <- findClosestTimeStamp(segments$station_id[i], segments$start_timestamp[i], videos)
}
segments$url %>% unique() %>% paste(collapse=",") %>% write(file="copycommand.txt")
library(stringr)
segments$segmentLength <- str_extract_all(segments$text, "\\s") %>% lapply(FUN = length) %>% unlist
uniqueSegments <- segments %>%
group_by(cluster) %>%
arrange(desc(segmentLength)) %>%
summarise(sampleText = text[1],
num_transcripts=n()) %>%
arrange(desc(num_transcripts))
write.csv(segments, file="./data/segments/07-25-2017_extracted-03-01.csv", row.names = F)
write.csv(uniqueSegments, file="./data/segments/07-25-2017_extracted-03-01_unique.csv", row.names = F)
json <- list()
for (i in 1:nrow(uniqueSegments)) {
cluster_to_process = uniqueSegments$cluster[i]
segment_list <- split(segments[segments$cluster == cluster_to_process, ] ,
seq(nrow(segments[segments$cluster == cluster_to_process, ]))) %>%
lapply(function(x) {return(x %>% as.list())}) %>%
unname() %>%
list() # cover with a "master list"
json[[i]] <- c("cluster_message"=uniqueSegments$sampleText[i], "stations"=segment_list)
}
json <- list("date"="07-25-2017", "clusters"=json)
json %>% jsonlite::toJSON(pretty = T, auto_unbox = T) %>% write_lines('./data/segments/07-25-2017_extracted-03-06-18.json')
View(uniqueSegments)
ggg <- similarityScoresNgram > 0.7
library(igraph)
g1 <- graph_from_adjacency_matrix(ggg, mode = "directed")
cluster <- clusters(g1, mode = "weak")$membership
content$cluster <- cluster
plot(g1, mode = "weak", edge.arrow.size=0.1,
vertex.size=5,
vertex.color=cluster,
vertex.label=NA)
getVideoQuery <- function(station_id, start_timestamp) {
timestamps <- lubridate::mdy_hms(segments$start_timestamp)
station_id %>%
stringr::str_extract('.*?(?=-)') %>%
paste(lubridate::month(timestamps) %>%
str_pad(2, "left", "0"),
lubridate::day(timestamps) %>%
str_pad(2, "left", "0"), lubridate::year(timestamps) %>% substr(3,4), sep = "") %>%
{paste('FileName LIKE "%', ., '%" OR ', sep = "")} %>%
unique() %>%
paste(collapse = " ")
}
write_lines(getVideoQuery(segments$station_id, segments$start_timestamp), "test.txt")
segments <- content[order(content$cluster), c("num_id", "contentId", "text", "cluster", "start_line",
"end_line", "start_timestamp", "end_timestamp", "station_id")]
segments$url <- ""
library(stringr)
segments$segmentLength <- str_extract_all(segments$text, "\\s") %>% lapply(FUN = length) %>% unlist
uniqueSegments <- segments %>%
group_by(cluster) %>%
arrange(desc(segmentLength)) %>%
summarise(sampleText = text[1],
num_transcripts=n()) %>%
arrange(desc(num_transcripts))
write.csv(segments, file="./data/segments/07-25-2017_extracted-03-01.csv", row.names = F)
write.csv(uniqueSegments, file="./data/segments/07-25-2017_extracted-03-01_unique.csv", row.names = F)
library(jsonlite)
library(readr)
json <- list()
for (i in 1:nrow(uniqueSegments)) {
cluster_to_process = uniqueSegments$cluster[i]
segment_list <- split(segments[segments$cluster == cluster_to_process, ] ,
seq(nrow(segments[segments$cluster == cluster_to_process, ]))) %>%
lapply(function(x) {return(x %>% as.list())}) %>%
unname() %>%
list() # cover with a "master list"
json[[i]] <- c("cluster_message"=uniqueSegments$sampleText[i], "stations"=segment_list)
}
json <- list("date"="07-25-2017", "clusters"=json)
json %>% jsonlite::toJSON(pretty = T, auto_unbox = T) %>% write_lines('./data/segments/07-25-2017_extracted-03-06-18.json')
View(uniqueSegments)
getVideoQuery <- function(station_id, start_timestamp) {
timestamps <- lubridate::mdy_hms(segments$start_timestamp)
station_id %>%
stringr::str_extract('.*?(?=-)') %>%
paste(lubridate::month(timestamps) %>%
str_pad(2, "left", "0"),
lubridate::day(timestamps) %>%
str_pad(2, "left", "0"), lubridate::year(timestamps) %>% substr(3,4), sep = "") %>%
{paste('FileName LIKE "%', ., '%" OR ', sep = "")} %>%
unique() %>%
paste(collapse = " ")
}
write_lines(getVideoQuery(segments$station_id, segments$start_timestamp), "test.txt")
x1 = 1
x2 = 2
sprintf("Can have multiple %s occurrences %s", x1, x2, "- got it?")
list.in <- function(list) {
return(paste(list, collapse=","))
}
list.in(c(1,2,3,4,5))
"bi-partisan" %>% str_replace_all("(?<=[[:alnum:]])-(?=[[:alnum:]])", "")
"bi---partisan" %>% str_replace_all("(?<=[[:alnum:]])-(?=[[:alnum:]])", "")
"bi-part-isan" %>% str_replace_all("(?<=[[:alnum:]])-(?=[[:alnum:]])", "")
"bi---partisan" %>% str_replace_all("(?<=[[:alnum:]])-*(?=[[:alnum:]])", "")
as.Date("2017-07-25") + 1
sprintf("%s", "123")
print("%s")
data.frame(Segment_ID = character(0),
words = character(0),
LineNumber = character(0),
LineDateTime = character(0),
stringsAsFactors = F)
